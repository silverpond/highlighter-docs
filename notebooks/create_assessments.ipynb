{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "import urllib\n",
    "import tarfile\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "\n",
    "import highlighter_client_v2 as hl\n",
    "from highlighter_client_v2.datasets import ImageRecord, AttributeRecord, Dataset\n",
    "\n",
    "from IPython.display import display_html\n",
    "from itertools import chain,cycle\n",
    "\n",
    "def show_dataset(ds):\n",
    "    \"\"\"Helper to display Datasets nicely in the Notebook\n",
    "    \"\"\"\n",
    "    html_str=''\n",
    "    for df,title in zip([ds.annotations_df, ds.images_df], chain([\"Annotations\", \"Images\"],cycle(['</br>'])) ):\n",
    "        html_str+='<th style=\"text-align:center\"><td style=\"vertical-align:top\">'\n",
    "        html_str+=f'<h2 style=\"text-align: center;\">{title}</h2>'\n",
    "        html_str+=df.head(5).to_html().replace('table','table style=\"display:inline\"')\n",
    "        html_str+=f'<br> shape: {df.shape}</td></th>'\n",
    "    display_html(html_str,raw=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "SAMPLE_DATASET_URL = \"https://highlighter-public.s3.ap-southeast-2.amazonaws.com/simple-shapes-coco/simple_shapes_dataset.tar\"\n",
    "\n",
    "# Create a temporary directory\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "try:\n",
    "    # Download the tar file\n",
    "    filename = SAMPLE_DATASET_URL.split('/')[-1]\n",
    "    filepath = Path(temp_dir) / filename\n",
    "    urllib.request.urlretrieve(SAMPLE_DATASET_URL, filepath)\n",
    "\n",
    "    # Extract the tar file\n",
    "    with tarfile.open(filepath, 'r') as tar:\n",
    "        tar.extractall(temp_dir)\n",
    "        \n",
    "    dataset_path = Path(temp_dir) / filepath.stem\n",
    "    print(\"File downloaded and extracted to:\", dataset_path)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "    \n",
    "COCO_JSON = dataset_path / \"data.json\"\n",
    "IMAGES_DIR = dataset_path / \"images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Create a Dataset From A Supported Format\n",
    "\n",
    "Some common dataset formats can be read from out-of-the-box, and we plan to add more as time goes on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = Dataset.read_coco(COCO_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Initalize A Highlighter Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_token = os.environ[\"HL_WEB_GRAPHQL_API_TOKEN\"]\n",
    "endpoint_url = os.environ[\"HL_WEB_GRAPHQL_ENDPOINT\"]\n",
    "\n",
    "client = hl.HLClient.from_credential(api_token=api_token, endpoint_url=endpoint_url)\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Upload The Images To A Data Source\n",
    "\n",
    "**First create a Data Source in the Highlighter Web UI, note the id and come back**\n",
    "\n",
    "You can find the ID in the URL\n",
    "\n",
    "```\n",
    "https://compuglobalhypermeganet.highlighter.ai/data_sources/#####\n",
    "                                                            ^^^^^\n",
    "                                                              |\n",
    "                                Data Source ID -----------------\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_source_id = 2601 # ToDo\n",
    "\n",
    "_ = ds.upload_images(client, data_source_id, image_dir=IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Create Object Classes\n",
    "\n",
    "Here we map the class names in the source dataset to Highlighter ObjectClass uuids. We will create\n",
    "them in Highlighter if one of the same name does not already exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the unique object class names\n",
    "adf = ds.annotations_df\n",
    "object_class_names = adf[adf.attribute_id == str(hl.OBJECT_CLASS_ATTRIBUTE_UUID)].value.unique()\n",
    "\n",
    "# This function checks if object classes exist by of the same name\n",
    "# and is case incentive before creating them. Then returns a dict mapping\n",
    "# the original name to the Highlighter ObjectClass.uuid\n",
    "object_class_name_to_highlighter_uuid = hl.object_classes.create_object_classes(client, object_class_names)\n",
    "print(object_class_name_to_highlighter_uuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Create An Assessment Process\n",
    "\n",
    "The Assessment process is where we store the annotations for a set of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you already have a process_id set it here, if not leave as None\n",
    "process_id = 987\n",
    "\n",
    "if process_id is None:\n",
    "\n",
    "    # Create an Assessment Process\n",
    "    # Note: Assessment processes names must be unique\n",
    "    process_name = \"My Toy Process 000\"\n",
    "\n",
    "    assessment_process = hl.create_assessment_process(client, name=process_name,\n",
    "                             object_class_uuids=[str(i) for i in object_class_name_to_highlighter_uuid.values()])\n",
    "    process_id = assessment_process.id\n",
    "    print(assessment_process)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from highlighter_client_v2.datasets.formats.highlighter.writer import HighlighterSubmissionsWriter\n",
    "\n",
    "# Define the Dataset Writer\n",
    "writer = HighlighterSubmissionsWriter(client=client,\n",
    "                                      assessment_process_id=process_id,\n",
    "                                        )\n",
    "\n",
    "writer.write(dataset=ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.object_class_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "**Your data should now be visible in the Assesment Process you defined**\n",
    "\n",
    "Below are some extra credit tutorials\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "# Create Dataset From A Custom Format\n",
    "\n",
    "Many times you will be uploading data from a non standard format. The dataset we're working with is in \n",
    "the popular Coco format which **is** supported by Highlighter. However, for the purpose of the exercisewe'll\n",
    "do this manually.\n",
    "\n",
    "The below code block loops through each image and creates a list of `ImagRecord`s then loops through each annotation and creates a list of `AttributeRecord`s. The `ImageRecord`s are pretty straight forward, but let us focus on the `AttributeRecord`s\n",
    "\n",
    "In its simplest form each `AttributeRecord` requres:\n",
    "  - `image_id`: This indicates the image the attribute belongs to\n",
    "  - `value`: This is the value of the attribute, and\n",
    "  - `entity_id`: This uniquely identifies an individual object or \"thing\" in an image or even across time or data sources. For example, in the block below we delibrately use the same `entity_id` for both the `PixelLocationAttributeValue` and `ObjectClassAttributeValue`. This tells Highlighter both attributes refer to the same \"thing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from highlighter_client_v2 import read_object_classes, LabeledUUID\n",
    "from highlighter_client_v2.datasets.base_models import (\n",
    "    ObjectClassAttributeValue,\n",
    "    PixelLocationAttributeValue,\n",
    "    AttributeRecord,\n",
    "    ImageRecord\n",
    ")\n",
    "\n",
    "with open(COCO_JSON, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "# Get a lookup to map class names to object class uuids\n",
    "object_class_uuid_lookup = {o.name: o.uuid for o in read_object_classes(client, process_id=process_id)}\n",
    "cat_id_to_name = {c[\"id\"]: c[\"name\"] for c in data[\"categories\"]}\n",
    "\n",
    "# We use the ImageRecord BaseModel to validate the fields\n",
    "# before adding them to the Dataset.\n",
    "image_records = [ImageRecord(image_id=i[\"id\"],\n",
    "                             width=i[\"width\"],\n",
    "                             height=i[\"height\"],\n",
    "                             filename=i[\"file_name\"],\n",
    "                            ) for i in data[\"images\"]]\n",
    "\n",
    "attribute_records = []\n",
    "for a in data[\"annotations\"]:\n",
    "    entity_id = str(uuid4())\n",
    "    \n",
    "    # Create an AttributeRecord with an ObjectClassAttributeValue by:\n",
    "    #   - looking up the object_class_uuid from a dict\n",
    "    #   - creating an LabeledUUID for the object class value. You can use LabeledUUID\n",
    "    #     or UUID interchangably. LabeledUUID is simply used to make things readable\n",
    "    #   - Append the AttributeRecord to attribute_records\n",
    "    object_class_name = cat_id_to_name[a[\"category_id\"]]\n",
    "    object_class_uuid = object_class_uuid_lookup[object_class_name]\n",
    "    object_class_value = LabeledUUID(object_class_uuid, label=object_class_name)\n",
    "    object_class_attribute_value = ObjectClassAttributeValue(value=object_class_value)\n",
    "    \n",
    "    attribute_records.append(\n",
    "        AttributeRecord.from_attribute_value(\n",
    "            a[\"image_id\"],\n",
    "            object_class_attribute_value,\n",
    "            entity_id=entity_id,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Create an AttributeRecord with an PixelLocationAttributeValue by:\n",
    "    #   - using the PixelLocationAttributeValue helper function to from_left_top_width_height_coords\n",
    "    #   - Append the AttributeRecord to attribute_records\n",
    "    pixel_location_attribute_value = PixelLocationAttributeValue.from_left_top_width_height_coords(a[\"bbox\"])\n",
    "    \n",
    "    # Create an PixelLocation AttributeValue\n",
    "    attribute_records.append(\n",
    "        AttributeRecord.from_attribute_value(\n",
    "            a[\"image_id\"],\n",
    "            pixel_location_attribute_value,\n",
    "            entity_id=entity_id,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Populate the Dataset\n",
    "ds = Dataset(attribute_records=attribute_records, image_records=image_records)\n",
    "\n",
    "show_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Once the `Dataset` object has been created you can use the same steps details at the beginning of the Notebook to upload the images and  attributes.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# Create Submissions By Performing Inference On Images In Highlighter\n",
    "\n",
    "Finally. If you have images alread stored in Highligher and you want to do predictions on those images and upload the results to Highligher you can follow a simplar process, but without the needing to create `ImageRecords` becuause the images are already in Highlighter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCrappyShapePredictor():\n",
    "    \n",
    "    def __init__(self, object_class_uuids):\n",
    "        self.object_class_uuids = object_class_uuids\n",
    "        \n",
    "    def predict(self, image):\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
