{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "vfPIxwiSLfmh"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "ZDTdnWdTJKt0"
   },
   "source": [
    "<div align=\"left\">\n",
    "    <img width=\"640\" src=\"https://highlighter-public.s3.ap-southeast-2.amazonaws.com/web/assets/Highlighter_Logo_Primary_Horizontal_RGB.png\" alt=\"Highlighter logo\">\n",
    "</div>\n",
    "\n",
    "# Create Assessments\n",
    "\n",
    "An `assessment` is a collections of *attributes* associated with a `data_file`. In the follwing example we're dealing with a very small object detection dataset in the common `Coco` format. So each `data_file` is an image in the dataset and the collection of bounding boxes associated with each image is the `assessment`.\n",
    "\n",
    "In the real world `assessment`s can be created in several different ways. This notebook demos a few common usecases:\n",
    "\n",
    "  - You have an existing dataaset in Coco format, you wish to upload the image and the annotations.\n",
    "  - You have an existing dataset in some custom format the is not currently supported by the `highlighter-sdk`\n",
    "  - You have some local process like a deep learning model you run locally to produce outputs you wish to upload as `assessment`s in Highligher\n",
    "\n",
    "---\n",
    "\n",
    "First we need to install the `highlighter-sdk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZEcegPiHUCx",
    "outputId": "f1df457c-0829-4a4d-dd48-02040239698b"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet highlighter-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "jnt5idmsL0KL"
   },
   "source": [
    "# Imports and Download Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "import urllib\n",
    "import tarfile\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "\n",
    "import highlighter as hl\n",
    "from highlighter.datasets import ImageRecord, AttributeRecord, Dataset\n",
    "\n",
    "from IPython.display import display_html\n",
    "from itertools import chain,cycle\n",
    "\n",
    "def show_dataset(ds):\n",
    "    \"\"\"Helper to display Datasets nicely in the Notebook\n",
    "    \"\"\"\n",
    "    html_str=''\n",
    "    for df,title in zip([ds.annotations_df, ds.data_files_df], chain([\"Annotations\", \"Data Files\"],cycle(['</br>'])) ):\n",
    "        html_str+='<th style=\"text-align:center\"><td style=\"vertical-align:top\">'\n",
    "        html_str+=f'<h2 style=\"text-align: center;\">{title}</h2>'\n",
    "        html_str+=df.head(5).to_html().replace('table','table style=\"display:inline\"')\n",
    "        html_str+=f'<br> shape: {df.shape}</td></th>'\n",
    "    display_html(html_str,raw=True)\n",
    "\n",
    "\n",
    "SAMPLE_DATASET_URL = \"https://highlighter-public.s3.ap-southeast-2.amazonaws.com/simple-shapes-coco/simple_shapes_dataset.tar\"\n",
    "\n",
    "TEMP_DIR = Path(tempfile.mkdtemp())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_sample_data(temp_dir=TEMP_DIR):\n",
    "    dataset_path = TEMP_DIR / \"sample_dataset\"\n",
    "    coco_json = dataset_path / \"data.json\"\n",
    "    data_files_dir = dataset_path / \"images\"\n",
    "\n",
    "    if coco_json.exists():\n",
    "        print(f\"Existing data found at: {dataset_path}\")\n",
    "        return coco_json, data_files_dir\n",
    "\n",
    "    try:\n",
    "        # Download the tar file\n",
    "        filename = SAMPLE_DATASET_URL.split('/')[-1]\n",
    "        filepath = Path(temp_dir) / filename\n",
    "        urllib.request.urlretrieve(SAMPLE_DATASET_URL, filepath)\n",
    "\n",
    "        # Extract the tar file\n",
    "        with tarfile.open(filepath, 'r') as tar:\n",
    "            tar.extractall(temp_dir)\n",
    "\n",
    "        (Path(temp_dir) / filepath.stem).rename(dataset_path)\n",
    "        print(f\"File downloaded and extracted to: {dataset_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    return coco_json, data_files_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "2"
   },
   "source": [
    "# Create a Dataset From A Supported Format\n",
    "\n",
    "Some common dataset formats can be read from out-of-the-box, and we plan to add more as time goes on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3",
    "outputId": "4a09d860-0cea-47cd-fef8-83ad1b9f8366",
    "tags": []
   },
   "outputs": [],
   "source": [
    "COCO_JSON, DATA_FILES_DIR = get_sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = Dataset.read_coco(COCO_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "5",
    "outputId": "336bbf20-a0f6-4cec-ddbf-4e40f7f3023f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "6"
   },
   "source": [
    "# Initalize A Highlighter Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7",
    "outputId": "a7e396ed-af38-455d-a026-a44d6e593871",
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_token = \"ADD API TOKEN HERE\"  # See https://highlighter-docs.netlify.app/docs/how-to-guides/highlighter-credentials/ for more info\n",
    "endpoint_url = \"https://<YOUR ACCOUNT SUB DOMAIN>.highlighter.ai/graphql\"\n",
    "\n",
    "\n",
    "client = hl.HLClient.from_credential(api_token=api_token, endpoint_url=endpoint_url)\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "8"
   },
   "source": [
    "# Upload The Images To A Data Source\n",
    "\n",
    "**First create a Data Source in the Highlighter Web UI, note the id and come back**\n",
    "\n",
    "You can find the ID in the URL\n",
    "\n",
    "```\n",
    "https://compuglobalhypermeganet.highlighter.ai/data_sources/#####\n",
    "                                                            ^^^^^\n",
    "                                                              |\n",
    "                                Data Source ID -----------------\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9",
    "outputId": "40eccb4a-a981-4cee-9dbb-25b48082a778",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_source_id = ToDo\n",
    "\n",
    "_ = ds.upload_data_files(client, data_source_id, data_file_dir=DATA_FILES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "10",
    "outputId": "3515b814-4667-46d3-fb16-ff1a1c86823f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "id": "11"
   },
   "source": [
    "# Create Object Classes\n",
    "\n",
    "Here we map the class names in the source dataset to Highlighter ObjectClass uuids. We will create\n",
    "them in Highlighter if one of the same name does not already exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12",
    "outputId": "ff6adfba-e271-45b4-cc5d-401781611e60",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the unique object class names\n",
    "adf = ds.annotations_df\n",
    "object_class_names = adf[adf.attribute_id == str(hl.OBJECT_CLASS_ATTRIBUTE_UUID)].value.unique()\n",
    "\n",
    "# This function checks if object classes exist by of the same name\n",
    "# and is case incentive before creating them. Then returns a dict mapping\n",
    "# the original name to the Highlighter ObjectClass.uuid\n",
    "object_class_name_to_highlighter_uuid = hl.object_classes.create_object_classes(client, object_class_names)\n",
    "print(object_class_name_to_highlighter_uuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "id": "13"
   },
   "source": [
    "# Create Workflow\n",
    "The Workflow is where we store the annotations for a set of `data_files`. *In our case these data_files are images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "id": "14",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you already have a workflow_id set it here, if not leave as None\n",
    "workflow_id = ToDo\n",
    "\n",
    "if workflow_id is None:\n",
    "\n",
    "    # Create an Workflow\n",
    "    # Note: Workflow names must be unique\n",
    "    workflow_name = \"My Toy workflow 000\"\n",
    "\n",
    "    workflow = hl.create_workflow(client, name=workflow_name,\n",
    "                             object_class_uuids=[str(i) for i in object_class_name_to_highlighter_uuid.values()])\n",
    "    workflow_id = workflow.id\n",
    "    print(workflow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "id": "15",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from highlighter.datasets.formats.highlighter.writer import HighlighterAssessmentsWriter\n",
    "\n",
    "# Define the Dataset Writer\n",
    "writer = HighlighterAssessmentsWriter(client,\n",
    "                                      workflow_id,\n",
    "                                      object_class_uuid_lookup=object_class_name_to_highlighter_uuid\n",
    "                                      )\n",
    "\n",
    "writer.write(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "id": "16"
   },
   "outputs": [],
   "source": [
    "ds.annotations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "17"
   },
   "source": [
    "**Your data should now be visible in the Workflow you defined**\n",
    "\n",
    "Below are some extra credit tutorials\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "18"
   },
   "source": [
    "# Create Dataset From A Custom Format\n",
    "\n",
    "Many times you will be uploading data from a non standard format. The dataset we're working with is in\n",
    "the popular Coco format which **is** supported by Highlighter. However, for the purpose of the exercisewe'll\n",
    "do this manually.\n",
    "\n",
    "The below code block loops through each image and creates a list of `ImagRecord`s then loops through each annotation and creates a list of `AttributeRecord`s. The `ImageRecord`s are pretty straight forward, but let us focus on the `AttributeRecord`s\n",
    "\n",
    "In its simplest form each `AttributeRecord` requres:\n",
    "  - `data_file_id`: This indicates the image the attribute belongs to\n",
    "  - `value`: This is the value of the attribute, and\n",
    "  - `entity_id`: This uniquely identifies an individual object or \"thing\" in an image or even across time or data sources. For example, in the block below we delibrately use the same `entity_id` for both the `PixelLocationAttributeValue` and `ObjectClassAttributeValue`. This tells Highlighter both attributes refer to the same \"thing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "id": "19"
   },
   "outputs": [],
   "source": [
    "COCO_JSON, DATA_FILES_DIR = get_sample_data()\n",
    "\n",
    "api_token = os.environ[\"HL_WEB_GRAPHQL_API_TOKEN\"]\n",
    "endpoint_url = os.environ[\"HL_WEB_GRAPHQL_ENDPOINT\"]\n",
    "client = hl.HLClient.from_credential(api_token=api_token, endpoint_url=endpoint_url)\n",
    "print(client)\n",
    "\n",
    "workflow_id = ToDo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "id": "20"
   },
   "outputs": [],
   "source": [
    "from highlighter import read_object_classes, LabeledUUID\n",
    "from highlighter.datasets.base_models import (\n",
    "    ObjectClassAttributeValue,\n",
    "    PixelLocationAttributeValue,\n",
    "    AttributeRecord,\n",
    "    ImageRecord\n",
    ")\n",
    "\n",
    "with open(COCO_JSON, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Get a lookup to map class names to object class uuids\n",
    "object_class_uuid_lookup = {o.name: o.uuid for o in read_object_classes(client, process_id=workflow_id)}\n",
    "cat_id_to_name = {c[\"id\"]: c[\"name\"] for c in data[\"categories\"]}\n",
    "\n",
    "# We use the ImageRecord BaseModel to validate the fields\n",
    "# before adding them to the Dataset.\n",
    "data_file_records = [ImageRecord(data_file_id=i[\"id\"],\n",
    "                             width=i[\"width\"],\n",
    "                             height=i[\"height\"],\n",
    "                             filename=i[\"file_name\"],\n",
    "                            ) for i in data[\"images\"]]\n",
    "\n",
    "attribute_records = []\n",
    "for a in data[\"annotations\"]:\n",
    "    entity_id = str(uuid4())\n",
    "\n",
    "    # Create an AttributeRecord with an ObjectClassAttributeValue by:\n",
    "    #   - looking up the object_class_uuid from a dict\n",
    "    #   - creating an LabeledUUID for the object class value. You can use LabeledUUID\n",
    "    #     or UUID interchangably. LabeledUUID is simply used to make things readable\n",
    "    #   - Append the AttributeRecord to attribute_records\n",
    "    object_class_name = cat_id_to_name[a[\"category_id\"]]\n",
    "    object_class_uuid = object_class_uuid_lookup[object_class_name]\n",
    "    object_class_value = LabeledUUID(object_class_uuid, label=object_class_name)\n",
    "    object_class_attribute_value = ObjectClassAttributeValue(value=object_class_value)\n",
    "\n",
    "    attribute_records.append(\n",
    "        AttributeRecord.from_attribute_value(\n",
    "            a[\"image_id\"],\n",
    "            object_class_attribute_value,\n",
    "            entity_id=entity_id,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create an AttributeRecord with an PixelLocationAttributeValue by:\n",
    "    #   - using the PixelLocationAttributeValue helper function to from_left_top_width_height_coords\n",
    "    #   - Append the AttributeRecord to attribute_records\n",
    "    pixel_location_attribute_value = PixelLocationAttributeValue.from_left_top_width_height_coords(a[\"bbox\"])\n",
    "\n",
    "    # Create an PixelLocation AttributeValue\n",
    "    attribute_records.append(\n",
    "        AttributeRecord.from_attribute_value(\n",
    "            a[\"image_id\"],\n",
    "            pixel_location_attribute_value,\n",
    "            entity_id=entity_id,\n",
    "        )\n",
    "    )\n",
    "\n",
    "ds = Dataset(attribute_records=attribute_records, data_file_records=data_file_records)\n",
    "\n",
    "# Upload files as needed and update data_file_ids\n",
    "_ = ds.upload_data_files(client, data_source_id, data_file_dir=DATA_FILES_DIR)\n",
    "\n",
    "show_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "id": "21"
   },
   "outputs": [],
   "source": [
    "from highlighter.datasets.formats.highlighter.writer import HighlighterAssessmentsWriter\n",
    "\n",
    "# Define the Dataset Writer\n",
    "writer = HighlighterAssessmentsWriter(client,\n",
    "                                      workflow_id)\n",
    "writer.write(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "22"
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Create Submissions By Performing Inference On Images In Highlighter\n",
    "\n",
    "Finally. If you have images alread stored in Highligher and you want to do predictions on those images and upload the results to Highligher you can follow a similar process, but without the needing to create `ImageRecords` becuause the images are already in Highlighter.\n",
    "\n",
    "We assume we're looping over a directory of image files with their filename matching their Highlighter `data_file_id`. To set this up we're going to create a directory containing symlinks `<data_file_id>.jpg` that refer to the original image paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "id": "23"
   },
   "outputs": [],
   "source": [
    "symlink_dir: Path = DATA_FILES_DIR.parent / \"hl_id_symlinks\"\n",
    "symlink_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for data_file_id, filename in ds.data_files_df.loc[:, [\"data_file_id\", \"filename\"]].values:\n",
    "    original_file_path = Path(filename)\n",
    "    link_path = symlink_dir / f\"{data_file_id}{original_file_path.suffix}\"\n",
    "    assert original_file_path.absolute().exists()\n",
    "    link_path.hardlink_to(original_file_path.absolute())\n",
    "\n",
    "!ls {symlink_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "id": "24"
   },
   "outputs": [],
   "source": [
    "COCO_JSON, DATA_FILES_DIR = get_sample_data()\n",
    "\n",
    "api_token = os.environ[\"HL_WEB_GRAPHQL_API_TOKEN\"]\n",
    "endpoint_url = os.environ[\"HL_WEB_GRAPHQL_ENDPOINT\"]\n",
    "client = hl.HLClient.from_credential(api_token=api_token, endpoint_url=endpoint_url)\n",
    "print(client)\n",
    "\n",
    "workflow_id = ToDo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "id": "25"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from uuid import uuid4\n",
    "from highlighter import read_object_classes, HLClient\n",
    "from highlighter.datasets.base_models import (\n",
    "    ObjectClassAttributeValue,\n",
    "    PixelLocationAttributeValue,\n",
    "    AttributeRecord,\n",
    "    ImageRecord\n",
    ")\n",
    "from highlighter import io\n",
    "\n",
    "\n",
    "class MyAwesomeShapePredictor():\n",
    "\n",
    "    def __init__(self, object_class_uuids):\n",
    "        self.object_class_uuids = object_class_uuids\n",
    "\n",
    "\n",
    "    def convert_output_to_attribute_records(self, raw_model_output: tuple, image_id: int) -> List[AttributeRecord]:\n",
    "\n",
    "        bbox, class_id, conf = raw_model_output\n",
    "\n",
    "        object_class_uuid = self.object_class_uuids[class_id]\n",
    "        object_class_attribute_value = ObjectClassAttributeValue(value=object_class_uuid,\n",
    "                                                                 confidence=conf)\n",
    "\n",
    "        bbox_attribtue_value = PixelLocationAttributeValue.from_left_top_width_height_coords(bbox,\n",
    "                                                                                             confidence=conf)\n",
    "\n",
    "        entity_id = uuid4()\n",
    "        attribute_records: List[AttributeRecord] = [\n",
    "\n",
    "            AttributeRecord.from_attribute_value(\n",
    "            image_id,\n",
    "            object_class_attribute_value,\n",
    "            entity_id=entity_id,\n",
    "            ),\n",
    "\n",
    "            AttributeRecord.from_attribute_value(\n",
    "            image_id,\n",
    "            bbox_attribtue_value,\n",
    "            entity_id=entity_id,\n",
    "            ),\n",
    "\n",
    "        ]\n",
    "\n",
    "        return attribute_records\n",
    "\n",
    "\n",
    "\n",
    "    def get_mock_predictions(self, image):\n",
    "\n",
    "        random_x = np.random.randint(0, image.shape[1]/2)\n",
    "        random_y = np.random.randint(0, image.shape[0]/2)\n",
    "        random_w = np.random.randint(0, image.shape[1]/2)\n",
    "        random_h = np.random.randint(0, image.shape[0]/2)\n",
    "        random_bbox = (random_x, random_y, random_w, random_h)\n",
    "\n",
    "        random_class_id = np.random.randint(low=0, high=len(self.object_class_uuids))\n",
    "        random_conf = np.random.uniform()\n",
    "        return (random_bbox, random_class_id, random_conf)\n",
    "\n",
    "    def predict(self, image_path: Path):\n",
    "        image_id = image_path.stem\n",
    "        image_np = io.read_image(image_path)\n",
    "        raw_model_output = self.get_mock_predictions(image_np)\n",
    "        attribute_records = self.convert_output_to_attribute_records(raw_model_output, image_id)\n",
    "        return attribute_records\n",
    "\n",
    "client = HLClient.from_env()\n",
    "object_class_uuids = [o.uuid for o in read_object_classes(client, process_id=workflow_id)]\n",
    "predictor = MyAwesomeShapePredictor(object_class_uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "id": "26"
   },
   "outputs": [],
   "source": [
    "\n",
    "attribute_records: List[AttributeRecord] = []\n",
    "for image_path in symlink_dir.glob(\"*.jpg\"):\n",
    "\n",
    "    records = predictor.predict(image_path)\n",
    "    attribute_records.extend(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "id": "27"
   },
   "outputs": [],
   "source": [
    "ds = Dataset(attribute_records=attribute_records)\n",
    "show_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "id": "28"
   },
   "outputs": [],
   "source": [
    "from highlighter.datasets.formats.highlighter.writer import HighlighterAssessmentsWriter\n",
    "\n",
    "# Define the Dataset Writer\n",
    "writer = HighlighterAssessmentsWriter(client,\n",
    "                                      workflow_id,\n",
    "                                        )\n",
    "\n",
    "writer.write(ds)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
