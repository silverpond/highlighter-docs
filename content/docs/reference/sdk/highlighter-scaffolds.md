+++
title = "Scaffolding a new agent"
description = "How to use Scaffolds to bootstrap your Highlighter agents"
date = 2025-02-27T08:00:00+00:00
updated = 2025-02-27T08:00:00+00:00
draft = false
weight = 100
sort_by = "weight"
template = "docs/page.html"

[extra]
lead = "How to use Scaffolds to bootstrap your Highlighter agents"
toc = true
top = false
+++

## Setup

If you haven't already, you should checkout [Getting Started With Highlighter SDK](../getting-started-with-highlighter-sdk/)


## Create a new Highlighter project scaffold

Highlighter project scaffolds provide a bunch of the boilerplate we all hate
when starting any new project.

```bash
# create a Highlighter project, follow the prompts
hl new .
```

## Create simple Agent

1. If you have not run `hl new .` to create a scaffold then do that first
  - This should have created a directory from the `title_slug` field of prompts, `cd` to that.
  - `pip install -e .`
2. `hl generate agent .`. This will:
  - create an `agents/` dir with an agent definition and a data source Capability
  for the data type specified in the prompts
  - create a `src/<title_slug>/<capability_name>.py` with a dummy implementation
  - add a `## Run Agent` section to your `README.md`
3. Run the command in the `## Run Agent` section of the `README.md`
## Create a new Capability and add it to the Agent
- Download some weights from Huggingface, https://huggingface.co/SpotLab/YOLOv8Detection/blob/3005c6751fb19cdeb6b10c066185908faf66a097/yolov8n.onnx
- Make a `weights` directory and move them to `weights/yolov8n.onnx`
- Create a new file in `src/???/capabilities/detector_capability.py`
- Follow the instructions in the below code snippet

```python
# Update your imports
from uuid import UUID
from highlighter.agent.capabilities.image_to_enum import OnnxYoloV8
# Add "MyPersonDetector" to the __all__, it should now have "MyPersonDetector"
# and the name of your original Capability that was generated by the
# `hl generate agent` script
# Add this class to the end of the file
# This class extends the base OnnxYoloV8 Capability by adding some default
# parameters and a log statement, you can do whatever you want later.
class MyPersonDetector(OnnxYoloV8):
    class DefaultStreamParameters(OnnxYoloV8.DefaultStreamParameters):
        num_classes: int = 1
        class_lookup: Optional[Dict[int, Tuple[UUID, str]]] = {0: (UUID(int=0), "person")}
        #conf_thresh: float = 0.1
        #nms_iou_thresh: float = 0.5
        is_absolute: bool = False
    def process_frame(self, stream, data_files: List[DataFile]) -> Tuple[StreamEvent, Dict]:
        stream_event, result = super().process_frame(stream, data_files)
        self.logger.info(f"processed: {data_files[0].media_frame_index} with {len(result['annotations'][0])} annotations")
        return stream_event, result
```
- Modify your agent definition
```
# Update the "graph" value
"graph": ["(Source MyPersonDetector ImageOverlay ImageWrite)"],
# Add this to your "elements" list
   {
      "name": "MyPersonDetector",
      "parameters": {
          "onnx_file": "weights/yolov8n.onnx"
      },
      "input": [
        {
          "name": "data_files",
          "type": "List[DataFile]"
        }
      ],
      "output": [
        {
          "name": "annotations",
          "type": "List[Annotation]"
        }
      ],
      "deploy": {
        "local": {
          "module": "my_awesome_project.capabilities"
        }
      }
    },
    {
      "name": "ImageOverlay",
      "parameters": {
      },
      "input": [
        {
          "name": "data_files",
          "type": "List[DataFile]"
        },
        {
          "name": "annotations",
          "type": "List[Annotation]"
        }
      ],
      "output": [
        {
          "name": "data_files",
          "type": "List[DataFile]"
        }
      ],
      "deploy": {
        "local": {
          "module": "highlighter.agent.capabilities.image_transforms"
        }
      }
    },
    {
      "name": "ImageWrite",
      "parameters": {
          "output_dir": "image_overlays",
          "output_pattern": "{media_frame_index}.jpg"
      },
      "input": [
        {
          "name": "data_files",
          "type": "List[DataFile]"
        }
      ],
      "output": [
      ],
      "deploy": {
        "local": {
          "module": "highlighter.agent.capabilities"
        }
      }
    }
```
## Run Detector Agent
**Run all scripts from the top level of the repo**
**Note: on Windows replace `/` with `\`**
- Create a directory called `image_overlays`
```
# process one file
hl agent run agents/YOU_AGENT_DEF.json -f VIDEO_PATH
```

### Training

## Training a model using the Highlighter SDK



Concepts

* **Highlighter Training Run**: A record in the Highlighter tool used to configure a training run.
* **Evaluation**: A page in the Highlighter tool used to record and track the performance of trained models.
* **Artefact**: A file containing model weights and accompanying information needed to run a model for inference.(Enables saving and tracking of trained models and results in Highlighter, ensuring others can access and reuse them without retraining.)
* **Highlighter SDK**: A python library including a CLI for interacting with Highlighter and doing common Highlighter related things, like training models.


To train a model using the SDK you must first setup the training run in the Highlighter Account. Then you can start the training run on the development machine using the Highlighter SDK

## Create New Training Run In Highlighter
1. Navigate to the Training tab in Highlighter
   ![training tab](content/docs/user-manual/resources/training_tab_image.png)

2. Click **Train a new model** or select an existing training run (we're going to assume you're training a new model)
3. Fill in the **Name** field with a meaningful name for your training run
4. Select the Capability the what the training run is for.
5. Select the **Evaluation** that will store the training run's evaluation metrics.
6. Select **Datasets**

   A training run must have at least one dataset for training and another of testing. There can be no overlay between the training  and test annotations.

   ![Dataset Sample](content/docs/user-manual/resources/dataset_sample.png)

7. **Model Template** can be ignored, leave as default.
2. **Config Override** can be ignored, leave as default.
3. Click **Save Training Run**.
4. Note the Training Run Id from the URL, (ie: https://highlighter-marketing-demo.highlighter.ai/training_runs/123).The training run id is 123



## Start Model Training Using The Highlighter SDK

1. 'hl generate training-run TRAINING_RUN_ID {yolo-det|yolo-seg|yolo-cls} ML_TRAINING_DIR'
   Generates a local training directory for a specified Highlighter training run.Downloads the configuration files, datasets, and model weights (for detection, segmentation, or classification) into the provided directory, allowing you to reproduce, resume, or evaluate the training locally.
```bash
hl generate training-run 123 yolo-seg .
```
Where:
  123: is the Training Run ID 
  yolo-seg : is the type of model to train
  . (dot): tells the sdk to create the directory (./123) in the current working directory that will store the training files

2. **OPTIONAL**:  Edit the cfg.yaml using your favourite text editor: eg: 'vim 123/cfg.yaml' .
3. Start training hl train start **TRAINING_RUN_DIR** , eg 'hl train start 123/' .
4. **OPTIONAL**: Run evaluation hl train evaluate **TRAINING_RUN_DIR CHECKPOINT CONFIG** , use the '--create' flag to upload the evaluation metrics to the Highlighter Evaluation linked to the Training Run.
5. **OPTIONAL**: Export the trained model and upload as a Highlighter Training Run **Artefact**.Eg: 'hl training-run artefact create -i 123 -a 123/runs/weights/artefact.yaml' .










